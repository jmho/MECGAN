{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "882032b4-eb34-4de5-991c-37b74c04628c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'inception_score'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01minception_score\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pyplot\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'inception_score'"
     ]
    }
   ],
   "source": [
    "#-*- coding: utf-8 -*-\n",
    "from __future__ import division\n",
    "import os\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from inception_score import *\n",
    "\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class ACGAN(object):\n",
    "    model_name = \"ACGAN\"     # name for checkpoint\n",
    "\n",
    "    def __init__(self, sess, epoch, batch_size, z_dim, dataset_name, checkpoint_dir, result_dir, log_dir):\n",
    "        self.sess = sess\n",
    "        self.dataset_name = dataset_name\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        self.result_dir = result_dir\n",
    "        self.log_dir = log_dir\n",
    "        self.epoch = epoch\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # load mnist\n",
    "        if dataset_name == 'cifar-10':                        \n",
    "            self.data_X, self.data_y = load_cifar(self.dataset_name) \n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "        # parameters\n",
    "        self.input_height = 32\n",
    "        self.input_width = 32\n",
    "        self.output_height = 32\n",
    "        self.output_width = 32\n",
    "\n",
    "        self.z_dim = z_dim         # dimension of noise-vector\n",
    "        self.y_dim = 10         # dimension of code-vector (label+two features)\n",
    "        self.c_dim = 3\n",
    "\n",
    "        # train\n",
    "        self.learning_rate_D = 0.0002\n",
    "        self.learning_rate_G = 0.001\n",
    "        self.learning_rate_Q = 0.0001\n",
    "        self.beta1 = 0.5\n",
    "\n",
    "        # test\n",
    "        self.sample_num = 64  # number of generated images to be saved\n",
    "\n",
    "        # code\n",
    "        self.len_discrete_code = 10  # categorical distribution (i.e. label)\n",
    "\n",
    "        # get number of batches for a single epoch\n",
    "        self.num_batches = len(self.data_X) // self.batch_size\n",
    "\n",
    "    def classifier(self, x, is_training=True, reuse=False):\n",
    "        # Network Architecture is exactly same as in infoGAN (https://arxiv.org/abs/1606.03657)\n",
    "        # Architecture : (64)5c2s-(128)5c2s_BL-FC1024_BL-FC128_BL-FC12Sâ€™\n",
    "        # All layers except the last two layers are shared by discriminator\n",
    "        with tf.variable_scope(\"classifier\", reuse=reuse):\n",
    "\n",
    "            net = lrelu(bn(linear(x, 128, scope='c_fc1'), is_training=is_training, scope='c_bn1'))\n",
    "            out_logit = linear(net, self.y_dim, scope='c_fc2')\n",
    "            out = tf.nn.softmax(out_logit)\n",
    "\n",
    "            return out, out_logit\n",
    "\n",
    "    def discriminator(self, x, is_training=True, reuse=False):\n",
    "        # Network Architecture is exactly same as in infoGAN (https://arxiv.org/abs/1606.03657)\n",
    "        # Architecture : (64)4c2s-(128)4c2s_BL-FC1024_BL-FC1_S\n",
    "        with tf.variable_scope(\"discriminator\", reuse=reuse):\n",
    "\n",
    "            net = lrelu(conv2d(x, 64, 4, 4, 2, 2, name='d_conv1'))\n",
    "            net = lrelu(bn(conv2d(net, 128, 4, 4, 2, 2, name='d_conv2'), is_training=is_training, scope='d_bn2'))\n",
    "            net = lrelu(bn(conv2d(net, 256, 4, 4, 2, 2, name='d_conv3'), is_training=is_training, scope='d_bn3'))\n",
    "            net = lrelu(conv2d(net, 1024, 4, 4, 4, 4, name='d_conv4'))\n",
    "            net = tf.reshape(net, [self.batch_size, -1])\n",
    "            out_logit = linear(net, 1, scope='d_fc5')\n",
    "            out = tf.nn.sigmoid(out_logit)\n",
    "\n",
    "            return out, out_logit, net\n",
    "\n",
    "    def generator(self, z, y, is_training=True, reuse=False):\n",
    "        # Network Architecture is exactly same as in infoGAN (https://arxiv.org/abs/1606.03657)\n",
    "        # Architecture : FC1024_BR-FC7x7x128_BR-(64)4dc2s_BR-(1)4dc2s_S\n",
    "        with tf.variable_scope(\"generator\", reuse=reuse):\n",
    "\n",
    "            # merge noise and code\n",
    "            z = concat([z, y], 1)\n",
    "\n",
    "            net = tf.nn.relu(bn(linear(z, 2*2*448, scope='g_fc1'), is_training=is_training, scope='g_bn1'))\n",
    "            net = tf.reshape(net, [self.batch_size, 2, 2, 448])\n",
    "            net = tf.nn.relu(bn(deconv2d(net, [self.batch_size, 4, 4, 256], 4, 4, 2, 2, name='g_dc2'), \n",
    "                                is_training=is_training,scope='g_bn2'))\n",
    "            net = tf.nn.relu(bn(deconv2d(net, [self.batch_size, 8, 8, 128], 4, 4, 2, 2, name='g_dc3'), \n",
    "                                is_training=is_training, scope='g_bn3'))\n",
    "            net = tf.nn.relu(bn(deconv2d(net, [self.batch_size, 16, 16, 64], 4, 4, 2, 2, name='g_dc4'), \n",
    "                                is_training=is_training, scope='g_bn4'))\n",
    "            out = tf.nn.sigmoid(deconv2d(net, [self.batch_size, 32, 32, 3], 4, 4, 2, 2, name='g_dc5'))\n",
    "\n",
    "            return out\n",
    "\n",
    "    def build_model(self):\n",
    "        # some parameters\n",
    "        image_dims = [self.input_height, self.input_width, self.c_dim]\n",
    "        bs = self.batch_size\n",
    "\n",
    "        \"\"\" Graph Input \"\"\"\n",
    "        # images\n",
    "        self.inputs = tf.placeholder(tf.float32, [bs] + image_dims, name='real_images')\n",
    "\n",
    "        # labels\n",
    "        self.y = tf.placeholder(tf.float32, [bs, self.y_dim], name='y')\n",
    "\n",
    "        # noises\n",
    "        self.z = tf.placeholder(tf.float32, [bs, self.z_dim], name='z')\n",
    "\n",
    "        \"\"\" Loss Function \"\"\"\n",
    "        ## 1. GAN Loss\n",
    "        # output of D for real images\n",
    "        D_real, D_real_logits, input4classifier_real = self.discriminator(self.inputs, is_training=True, reuse=False)\n",
    "\n",
    "        # output of D for fake images\n",
    "        G = self.generator(self.z, self.y, is_training=True, reuse=False)\n",
    "        D_fake, D_fake_logits, input4classifier_fake = self.discriminator(G, is_training=True, reuse=True)\n",
    "\n",
    "        # get loss for discriminator\n",
    "        d_loss_real = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(logits=D_real_logits, labels=tf.ones_like(D_real)))\n",
    "        d_loss_fake = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake_logits, labels=tf.zeros_like(D_fake)))\n",
    "\n",
    "        self.d_loss = d_loss_real + d_loss_fake\n",
    "\n",
    "        # get loss for generator\n",
    "        self.g_loss = tf.reduce_mean(\n",
    "            tf.nn.sigmoid_cross_entropy_with_logits(logits=D_fake_logits, labels=tf.ones_like(D_fake)))\n",
    "\n",
    "        ## 2. Information Loss\n",
    "        code_fake, code_logit_fake = self.classifier(input4classifier_fake, is_training=True, reuse=False)\n",
    "        code_real, code_logit_real = self.classifier(input4classifier_real, is_training=True, reuse=True)\n",
    "\n",
    "        # For real samples\n",
    "        q_real_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=code_logit_real, labels=self.y))\n",
    "\n",
    "        # For fake samples\n",
    "        q_fake_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=code_logit_fake, labels=self.y))\n",
    "\n",
    "        # get information loss\n",
    "        self.q_loss = q_fake_loss + q_real_loss\n",
    "\n",
    "        \"\"\" Training \"\"\"\n",
    "        # divide trainable variables into a group for D and a group for G\n",
    "        t_vars = tf.trainable_variables()\n",
    "        d_vars = [var for var in t_vars if 'd_' in var.name]\n",
    "        g_vars = [var for var in t_vars if 'g_' in var.name]\n",
    "        q_vars = [var for var in t_vars if ('d_' in var.name) or ('c_' in var.name) or ('g_' in var.name)]\n",
    "\n",
    "        # optimizers\n",
    "        with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "            self.d_optim = tf.train.AdamOptimizer(self.learning_rate_D, beta1=self.beta1) \\\n",
    "                .minimize(self.d_loss, var_list=d_vars)\n",
    "            self.g_optim = tf.train.AdamOptimizer(self.learning_rate_G, beta1=self.beta1) \\\n",
    "                .minimize(self.g_loss, var_list=g_vars)\n",
    "            self.q_optim = tf.train.AdamOptimizer(self.learning_rate_Q, beta1=self.beta1) \\\n",
    "                .minimize(self.q_loss, var_list=q_vars)\n",
    "\n",
    "        \"\"\"\" Testing \"\"\"\n",
    "        # for test\n",
    "        self.fake_images = self.generator(self.z, self.y, is_training=False, reuse=True)\n",
    "\n",
    "        \"\"\" Summary \"\"\"\n",
    "        d_loss_real_sum = tf.summary.scalar(\"d_loss_real\", d_loss_real)\n",
    "        d_loss_fake_sum = tf.summary.scalar(\"d_loss_fake\", d_loss_fake)\n",
    "        d_loss_sum = tf.summary.scalar(\"d_loss\", self.d_loss)\n",
    "        g_loss_sum = tf.summary.scalar(\"g_loss\", self.g_loss)\n",
    "\n",
    "        q_loss_sum = tf.summary.scalar(\"g_loss\", self.q_loss)\n",
    "        q_real_sum = tf.summary.scalar(\"q_real_loss\", q_real_loss)\n",
    "        q_fake_sum = tf.summary.scalar(\"q_fake_loss\", q_fake_loss)\n",
    "\n",
    "        # final summary operations\n",
    "        self.g_sum = tf.summary.merge([d_loss_fake_sum, g_loss_sum])\n",
    "        self.d_sum = tf.summary.merge([d_loss_real_sum, d_loss_sum])\n",
    "        self.q_sum = tf.summary.merge([q_loss_sum, q_real_sum, q_fake_sum])\n",
    "\n",
    "    def train(self):\n",
    "\n",
    "        # initialize all variables\n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "        # graph inputs for visualize training results\n",
    "        self.sample_z = np.random.uniform(-1, 1, size=(self.batch_size, self.z_dim))\n",
    "        self.test_codes = self.data_y[0:self.batch_size]\n",
    "\n",
    "        # saver to save model\n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "        # summary writer\n",
    "        self.writer = tf.summary.FileWriter(self.log_dir + '/' + self.model_name, self.sess.graph)\n",
    "\n",
    "        # restore check-point if it exits\n",
    "        could_load, checkpoint_counter = self.load(self.checkpoint_dir)\n",
    "        if could_load:\n",
    "            start_epoch = (int)(checkpoint_counter / self.num_batches)\n",
    "            start_batch_id = checkpoint_counter - start_epoch * self.num_batches\n",
    "            counter = checkpoint_counter\n",
    "            print(\" [*] Load SUCCESS\")\n",
    "        else:\n",
    "            start_epoch = 0\n",
    "            start_batch_id = 0\n",
    "            counter = 1\n",
    "            print(\" [!] Load failed...\")\n",
    "            \n",
    "        IS = []\n",
    "\n",
    "        # loop for epoch\n",
    "        start_time = time.time()\n",
    "        for epoch in range(start_epoch, self.epoch):\n",
    "\n",
    "            # get batch data\n",
    "            for idx in range(start_batch_id, self.num_batches):\n",
    "                batch_images = self.data_X[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "                batch_codes = self.data_y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "\n",
    "                batch_z = np.random.uniform(-1, 1, [self.batch_size, self.z_dim]).astype(np.float32)\n",
    "\n",
    "                # update D network\n",
    "                _, summary_str, d_loss = self.sess.run([self.d_optim, self.d_sum, self.d_loss],\n",
    "                                                       feed_dict={self.inputs: batch_images, self.y: batch_codes,\n",
    "                                                                  self.z: batch_z})\n",
    "                self.writer.add_summary(summary_str, counter)\n",
    "\n",
    "                # update G & Q network\n",
    "                _, summary_str_g, g_loss, _, summary_str_q, q_loss = self.sess.run(\n",
    "                    [self.g_optim, self.g_sum, self.g_loss, self.q_optim, self.q_sum, self.q_loss],\n",
    "                    feed_dict={self.z: batch_z, self.y: batch_codes, self.inputs: batch_images})\n",
    "                self.writer.add_summary(summary_str_g, counter)\n",
    "                self.writer.add_summary(summary_str_q, counter)\n",
    "\n",
    "                # display training status\n",
    "                counter += 1\n",
    "                if counter%500==0:\n",
    "                    print(\"Epoch: [%2d] [%4d/%4d] time: %4.4f, d_loss: %.8f, g_loss: %.8f, q_loss: %.8f\" \\\n",
    "                      % (epoch, idx, self.num_batches, time.time() - start_time, d_loss, g_loss, q_loss))\n",
    "\n",
    "                # save training results for every 300 steps\n",
    "                if np.mod(counter, 500) == 0:\n",
    "                    samples = self.sess.run(self.fake_images,\n",
    "                                            feed_dict={self.z: self.sample_z, self.y: self.test_codes})\n",
    "                    tot_num_samples = min(self.sample_num, self.batch_size)\n",
    "                    manifold_h = int(np.floor(np.sqrt(tot_num_samples)))\n",
    "                    manifold_w = int(np.floor(np.sqrt(tot_num_samples)))\n",
    "                    save_images(samples[:manifold_h * manifold_w, :, :, :], [manifold_h, manifold_w], './' + check_folder(\n",
    "                        self.result_dir + '/' + self.model_dir) + '/' + self.model_name + '_train_{:02d}_{:04d}.png'.format(\n",
    "                        epoch, idx))\n",
    "\n",
    "            # After an epoch, start_batch_id is set to zero\n",
    "            # non-zero value is only for the first epoch after loading pre-trained model\n",
    "            start_batch_id = 0\n",
    "\n",
    "            if epoch%5 == 0:\n",
    "                # save model\n",
    "                self.save(self.checkpoint_dir, counter)\n",
    "\n",
    "                # show temporal results\n",
    "                self.visualize_results(epoch)\n",
    "                \n",
    "                [a, b] = self.calculate_is()\n",
    "                print('\\n',a, b,'\\n')\n",
    "                IS.append(a)\n",
    "\n",
    "        # save model for final step\n",
    "        self.save(self.checkpoint_dir, counter)\n",
    "        \n",
    "        N = len(IS)\n",
    "        x = np.linspace(0, 5 * N - 5, N)\n",
    "        plt.plot(x, IS)\n",
    "        plt.xlabel('epoch') #Xè½´æ ‡ç­¾\n",
    "        plt.ylabel(\"IS\") #Yè½´æ ‡ç­¾\n",
    "        plt.savefig(check_folder(self.result_dir + '/' + self.model_dir) + '/' + self.model_name + '_epoch%03d' % epoch + 'IS.png',dpi = 900)\n",
    "\n",
    "    def visualize_results(self, epoch):\n",
    "        tot_num_samples = min(self.sample_num, self.batch_size)\n",
    "        image_frame_dim = int(np.floor(np.sqrt(tot_num_samples)))\n",
    "        z_sample = np.random.uniform(-1, 1, size=(self.batch_size, self.z_dim))\n",
    "\n",
    "        \"\"\" random noise, random discrete code, fixed continuous code \"\"\"\n",
    "        y = np.random.choice(self.len_discrete_code, self.batch_size)\n",
    "        y_one_hot = np.zeros((self.batch_size, self.y_dim))\n",
    "        y_one_hot[np.arange(self.batch_size), y] = 1\n",
    "\n",
    "        samples = self.sess.run(self.fake_images, feed_dict={self.z: z_sample, self.y: y_one_hot})\n",
    "\n",
    "        save_images(samples[:image_frame_dim*image_frame_dim,:,:,:], [image_frame_dim, image_frame_dim],\n",
    "                    check_folder(self.result_dir + '/' + self.model_dir) + '/' + self.model_name + '_test_all_classes.png')\n",
    "\n",
    "        \"\"\" specified condition, random noise \"\"\"\n",
    "        n_styles = 10  # must be less than or equal to self.batch_size\n",
    "\n",
    "        np.random.seed()\n",
    "        si = np.random.choice(self.batch_size, n_styles)\n",
    "\n",
    "        for l in range(self.len_discrete_code):\n",
    "            y = np.zeros(self.batch_size, dtype=np.int64) + l\n",
    "            y_one_hot = np.zeros((self.batch_size, self.y_dim))\n",
    "            y_one_hot[np.arange(self.batch_size), y] = 1\n",
    "\n",
    "            samples = self.sess.run(self.fake_images, feed_dict={self.z: z_sample, self.y: y_one_hot})\n",
    "#            save_images(samples[:image_frame_dim*image_frame_dim,:,:,:], [image_frame_dim, image_frame_dim],\n",
    "#                        check_folder(self.result_dir + '/' + self.model_dir) + '/' + self.model_name + '_epoch%03d' % epoch + '_test_class_%d.png' % l)\n",
    "\n",
    "            samples = samples[si, :, :, :]\n",
    "\n",
    "            if l == 0:\n",
    "                all_samples = samples\n",
    "            else:\n",
    "                all_samples = np.concatenate((all_samples, samples), axis=0)\n",
    "\n",
    "        \"\"\" save merged images to check style-consistency \"\"\"\n",
    "        canvas = np.zeros_like(all_samples)\n",
    "        for s in range(n_styles):\n",
    "            for c in range(self.len_discrete_code):\n",
    "                canvas[s * self.len_discrete_code + c, :, :, :] = all_samples[c * n_styles + s, :, :, :]\n",
    "\n",
    "        save_images(canvas, [n_styles, self.len_discrete_code],\n",
    "                    check_folder(self.result_dir + '/' + self.model_dir) + '/' + self.model_name + '_epoch%03d' % epoch + '_test_all_classes_style_by_style.png')\n",
    "        \n",
    "    def calculate_is(self):\n",
    "        imgs = np.zeros((((self.batch_size*100,32,32,3))))\n",
    "        for k in range(10):\n",
    "            for i in range(10):\n",
    "                y = np.zeros((self.batch_size, 10))\n",
    "                y[:, i] = 1\n",
    "                z = np.random.uniform(-1, 1, [self.batch_size, self.z_dim]).astype(np.float32)\n",
    "                #z = i*0.05 + np.random.normal(loc=0.0, scale=(1.0-0.0025*i*i), size=(self.batch_size,self.z_dim))\n",
    "                image = self.sess.run(self.fake_images, feed_dict = {self.z:z, self.y:y})\n",
    "                imgs[self.batch_size*10*k+self.batch_size*i:self.batch_size*10*k+self.batch_size*i+self.batch_size,:,:,:] = image\n",
    "        imgs = np.transpose(imgs, axes=[0, 3, 1, 2])\n",
    "        imgs = (imgs-0.5)*2\n",
    "        return (inception_score(imgs, cuda=True, batch_size=32, resize=True, splits=10))\n",
    "\n",
    "    @property\n",
    "    def model_dir(self):\n",
    "        return \"{}_{}_{}_{}\".format(\n",
    "            self.model_name, self.dataset_name,\n",
    "            self.batch_size, self.z_dim)\n",
    "\n",
    "    def save(self, checkpoint_dir, step):\n",
    "        checkpoint_dir = os.path.join(checkpoint_dir, self.model_dir, self.model_name)\n",
    "\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "\n",
    "        self.saver.save(self.sess,os.path.join(checkpoint_dir, self.model_name+'.model'), global_step=step)\n",
    "\n",
    "    def load(self, checkpoint_dir):\n",
    "        import re\n",
    "        print(\" [*] Reading checkpoints...\")\n",
    "        checkpoint_dir = os.path.join(checkpoint_dir, self.model_dir, self.model_name)\n",
    "\n",
    "        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
    "            self.saver.restore(self.sess, os.path.join(checkpoint_dir, ckpt_name))\n",
    "            counter = int(next(re.finditer(\"(\\d+)(?!.*\\d)\",ckpt_name)).group(0))\n",
    "            print(\" [*] Success to read {}\".format(ckpt_name))\n",
    "            return True, counter\n",
    "        else:\n",
    "            print(\" [*] Failed to find a checkpoint\")\n",
    "            return False, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d8d408-6857-4097-afed-d577d46c3ef8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow-2.6.0",
   "language": "python",
   "name": "tensorflow-2.6.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
